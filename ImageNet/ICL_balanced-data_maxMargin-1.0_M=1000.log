nohup: ignoring input
You are using config.init_device='cpu', but you can also use config.init_device="meta" with Composer + FSDP for fast initialization.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.07s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  4.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.17s/it]
Traceback (most recent call last):
  File "/data/chy/online/ImageNet/main.py", line 46, in <module>
    model, image_processor, tokenizer = create_model_and_transforms(
  File "/data/chy/online/open_flamingo_4/open_flamingo/src/factory.py", line 73, in create_model_and_transforms
    lang_encoder = AutoModelForCausalLM.from_pretrained(
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2065, in to
    return super().to(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 1; 23.68 GiB total capacity; 5.64 GiB already allocated; 33.75 MiB free; 5.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
