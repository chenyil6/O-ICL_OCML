nohup: ignoring input
/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().
  warnings.warn(_BETA_TRANSFORMS_WARNING)
/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().
  warnings.warn(_BETA_TRANSFORMS_WARNING)
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.05s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:03,  1.04it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:03<00:02,  1.10it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:04<00:01,  1.14it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:05<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.13it/s]
/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
load clip successfully...
Loaded class indices from ./imagenet_class_indices.pkl
get supportng set ...
get samples for each class:   0%|          | 0/100 [00:00<?, ?it/s]get samples for each class:   1%|          | 1/100 [00:00<00:31,  3.12it/s]get samples for each class:   2%|▏         | 2/100 [00:00<00:29,  3.27it/s]get samples for each class:   3%|▎         | 3/100 [00:00<00:27,  3.56it/s]get samples for each class:   4%|▍         | 4/100 [00:01<00:25,  3.75it/s]get samples for each class:   5%|▌         | 5/100 [00:01<00:26,  3.55it/s]get samples for each class:   6%|▌         | 6/100 [00:01<00:27,  3.37it/s]get samples for each class:   7%|▋         | 7/100 [00:02<00:29,  3.12it/s]get samples for each class:   8%|▊         | 8/100 [00:02<00:31,  2.95it/s]get samples for each class:   9%|▉         | 9/100 [00:02<00:31,  2.90it/s]get samples for each class:  10%|█         | 10/100 [00:03<00:31,  2.81it/s]get samples for each class:  11%|█         | 11/100 [00:03<00:29,  3.02it/s]get samples for each class:  12%|█▏        | 12/100 [00:03<00:27,  3.17it/s]get samples for each class:  13%|█▎        | 13/100 [00:04<00:26,  3.27it/s]get samples for each class:  14%|█▍        | 14/100 [00:04<00:26,  3.29it/s]get samples for each class:  15%|█▌        | 15/100 [00:04<00:25,  3.28it/s]get samples for each class:  16%|█▌        | 16/100 [00:04<00:25,  3.28it/s]get samples for each class:  17%|█▋        | 17/100 [00:05<00:24,  3.36it/s]get samples for each class:  18%|█▊        | 18/100 [00:05<00:25,  3.26it/s]get samples for each class:  19%|█▉        | 19/100 [00:05<00:25,  3.23it/s]get samples for each class:  20%|██        | 20/100 [00:06<00:24,  3.32it/s]get samples for each class:  21%|██        | 21/100 [00:06<00:23,  3.37it/s]get samples for each class:  22%|██▏       | 22/100 [00:06<00:22,  3.48it/s]get samples for each class:  23%|██▎       | 23/100 [00:07<00:22,  3.41it/s]get samples for each class:  24%|██▍       | 24/100 [00:07<00:22,  3.34it/s]get samples for each class:  25%|██▌       | 25/100 [00:07<00:22,  3.30it/s]get samples for each class:  26%|██▌       | 26/100 [00:07<00:22,  3.25it/s]get samples for each class:  27%|██▋       | 27/100 [00:08<00:21,  3.40it/s]get samples for each class:  28%|██▊       | 28/100 [00:08<00:21,  3.28it/s]get samples for each class:  29%|██▉       | 29/100 [00:08<00:21,  3.35it/s]get samples for each class:  30%|███       | 30/100 [00:09<00:18,  3.76it/s]get samples for each class:  31%|███       | 31/100 [00:09<00:20,  3.44it/s]get samples for each class:  32%|███▏      | 32/100 [00:09<00:20,  3.27it/s]get samples for each class:  33%|███▎      | 33/100 [00:10<00:21,  3.13it/s]get samples for each class:  34%|███▍      | 34/100 [00:10<00:20,  3.20it/s]get samples for each class:  35%|███▌      | 35/100 [00:10<00:20,  3.18it/s]get samples for each class:  36%|███▌      | 36/100 [00:11<00:20,  3.05it/s]get samples for each class:  37%|███▋      | 37/100 [00:11<00:20,  3.10it/s]get samples for each class:  38%|███▊      | 38/100 [00:12<00:26,  2.36it/s]get samples for each class:  39%|███▉      | 39/100 [00:12<00:23,  2.65it/s]get samples for each class:  40%|████      | 40/100 [00:12<00:22,  2.69it/s]get samples for each class:  41%|████      | 41/100 [00:12<00:20,  2.81it/s]get samples for each class:  42%|████▏     | 42/100 [00:13<00:19,  2.95it/s]get samples for each class:  43%|████▎     | 43/100 [00:13<00:18,  3.04it/s]get samples for each class:  44%|████▍     | 44/100 [00:13<00:17,  3.29it/s]get samples for each class:  45%|████▌     | 45/100 [00:14<00:16,  3.30it/s]get samples for each class:  46%|████▌     | 46/100 [00:14<00:16,  3.22it/s]get samples for each class:  47%|████▋     | 47/100 [00:14<00:16,  3.20it/s]get samples for each class:  48%|████▊     | 48/100 [00:15<00:15,  3.34it/s]get samples for each class:  49%|████▉     | 49/100 [00:15<00:16,  3.15it/s]get samples for each class:  50%|█████     | 50/100 [00:15<00:16,  2.97it/s]get samples for each class:  51%|█████     | 51/100 [00:16<00:16,  2.91it/s]get samples for each class:  52%|█████▏    | 52/100 [00:16<00:15,  3.09it/s]get samples for each class:  53%|█████▎    | 53/100 [00:16<00:15,  3.06it/s]get samples for each class:  54%|█████▍    | 54/100 [00:17<00:17,  2.70it/s]get samples for each class:  55%|█████▌    | 55/100 [00:17<00:16,  2.68it/s]get samples for each class:  56%|█████▌    | 56/100 [00:17<00:15,  2.77it/s]get samples for each class:  57%|█████▋    | 57/100 [00:18<00:15,  2.86it/s]get samples for each class:  58%|█████▊    | 58/100 [00:18<00:15,  2.77it/s]get samples for each class:  59%|█████▉    | 59/100 [00:19<00:15,  2.73it/s]get samples for each class:  60%|██████    | 60/100 [00:19<00:13,  3.06it/s]get samples for each class:  61%|██████    | 61/100 [00:19<00:12,  3.12it/s]get samples for each class:  62%|██████▏   | 62/100 [00:19<00:12,  2.98it/s]get samples for each class:  63%|██████▎   | 63/100 [00:20<00:12,  2.93it/s]get samples for each class:  64%|██████▍   | 64/100 [00:20<00:11,  3.21it/s]get samples for each class:  65%|██████▌   | 65/100 [00:21<00:12,  2.71it/s]get samples for each class:  66%|██████▌   | 66/100 [00:21<00:11,  2.84it/s]get samples for each class:  67%|██████▋   | 67/100 [00:21<00:11,  2.98it/s]get samples for each class:  68%|██████▊   | 68/100 [00:22<00:11,  2.90it/s]get samples for each class:  69%|██████▉   | 69/100 [00:22<00:09,  3.14it/s]get samples for each class:  70%|███████   | 70/100 [00:22<00:09,  3.07it/s]get samples for each class:  71%|███████   | 71/100 [00:22<00:09,  3.12it/s]get samples for each class:  72%|███████▏  | 72/100 [00:23<00:08,  3.15it/s]get samples for each class:  73%|███████▎  | 73/100 [00:23<00:08,  3.25it/s]get samples for each class:  74%|███████▍  | 74/100 [00:23<00:07,  3.49it/s]get samples for each class:  75%|███████▌  | 75/100 [00:24<00:07,  3.32it/s]get samples for each class:  76%|███████▌  | 76/100 [00:24<00:06,  3.44it/s]get samples for each class:  77%|███████▋  | 77/100 [00:24<00:06,  3.31it/s]get samples for each class:  78%|███████▊  | 78/100 [00:25<00:06,  3.31it/s]get samples for each class:  79%|███████▉  | 79/100 [00:25<00:05,  3.66it/s]get samples for each class:  80%|████████  | 80/100 [00:25<00:05,  3.44it/s]get samples for each class:  81%|████████  | 81/100 [00:25<00:04,  3.81it/s]get samples for each class:  82%|████████▏ | 82/100 [00:26<00:05,  3.56it/s]get samples for each class:  83%|████████▎ | 83/100 [00:26<00:05,  3.34it/s]get samples for each class:  84%|████████▍ | 84/100 [00:26<00:05,  3.19it/s]get samples for each class:  85%|████████▌ | 85/100 [00:27<00:04,  3.05it/s]get samples for each class:  86%|████████▌ | 86/100 [00:27<00:04,  3.02it/s]get samples for each class:  87%|████████▋ | 87/100 [00:27<00:04,  3.05it/s]get samples for each class:  88%|████████▊ | 88/100 [00:28<00:03,  3.11it/s]get samples for each class:  89%|████████▉ | 89/100 [00:28<00:03,  3.12it/s]get samples for each class:  90%|█████████ | 90/100 [00:28<00:03,  3.24it/s]get samples for each class:  91%|█████████ | 91/100 [00:28<00:02,  3.20it/s]get samples for each class:  92%|█████████▏| 92/100 [00:29<00:02,  3.25it/s]get samples for each class:  93%|█████████▎| 93/100 [00:29<00:02,  2.97it/s]get samples for each class:  94%|█████████▍| 94/100 [00:29<00:01,  3.07it/s]get samples for each class:  95%|█████████▌| 95/100 [00:30<00:01,  3.28it/s]get samples for each class:  96%|█████████▌| 96/100 [00:30<00:01,  3.39it/s]get samples for each class:  97%|█████████▋| 97/100 [00:30<00:00,  3.22it/s]get samples for each class:  98%|█████████▊| 98/100 [00:31<00:00,  3.21it/s]get samples for each class:  99%|█████████▉| 99/100 [00:31<00:00,  3.26it/s]get samples for each class: 100%|██████████| 100/100 [00:31<00:00,  3.00it/s]get samples for each class: 100%|██████████| 100/100 [00:31<00:00,  3.14it/s]
Support set size: 1000
Preprocess Supporting set...:   0%|          | 0/1000 [00:00<?, ?it/s]Preprocess Supporting set...: 100%|██████████| 1000/1000 [00:00<00:00, 380435.74it/s]
Inference ImageNet...:   0%|          | 0/5000 [00:00<?, ?it/s]Inference ImageNet...:   0%|          | 0/5000 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/data/chy/online/ImageNet/main.py", line 98, in <module>
    results, predictions = inferencer.run()
  File "/data/chy/online/ImageNet/inferencers.py", line 917, in run
    self.inference_batch(batch_samples)
  File "/data/chy/online/ImageNet/inferencers.py", line 775, in inference_batch
    self.evaluate_batch_on_idev2(batch_samples)
  File "/data/chy/online/ImageNet/inferencers.py", line 820, in evaluate_batch_on_idev2
    outputs = self.model.generate(**inputs, bad_words_ids=BAD_WORDS_IDS,min_new_tokens=1, max_new_tokens=20,num_beams=1,
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/generation/utils.py", line 2215, in generate
    result = self._sample(
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/generation/utils.py", line 3206, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/models/idefics2/modeling_idefics2.py", line 1624, in forward
    outputs = self.model(
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/models/idefics2/modeling_idefics2.py", line 1423, in forward
    image_hidden_states = self.vision_model(
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/models/idefics2/modeling_idefics2.py", line 706, in forward
    encoder_outputs = self.encoder(
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/models/idefics2/modeling_idefics2.py", line 554, in forward
    layer_outputs = encoder_layer(
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/models/idefics2/modeling_idefics2.py", line 466, in forward
    hidden_states, attn_weights = self.self_attn(
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/chy63/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/models/idefics2/modeling_idefics2.py", line 234, in forward
    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 316.00 MiB (GPU 0; 23.68 GiB total capacity; 2.60 GiB already allocated; 285.75 MiB free; 2.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
