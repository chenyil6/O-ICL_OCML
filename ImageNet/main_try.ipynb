{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from inferencers import *\n",
    "import logging\n",
    "from transformers import AutoTokenizer, CLIPModel,AutoProcessor\n",
    "import sys\n",
    "sys.path.append('/data/chy/online')\n",
    "from open_flamingo_v2.open_flamingo.src.factory import create_model_and_transforms\n",
    "from transformers import IdeficsForVisionText2Text, AutoProcessor\n",
    "import json\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_set = \"cuda:0\"\n",
    "device = torch.device(device_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "            clip_vision_encoder_path=\"ViT-L-14\",\n",
    "            clip_vision_encoder_pretrained=\"openai\",\n",
    "            lang_encoder_path=\"/data/share/mpt-7b/\",\n",
    "            tokenizer_path=\"/data/share/mpt-7b/\",\n",
    "            cross_attn_every_n_layers=4,\n",
    "            precision=\"fp16\",\n",
    "            inference=True,\n",
    "            device=device_set,\n",
    "            checkpoint_path=\"/data/share/OpenFlamingo-9B-vitl-mpt7b/checkpoint.pt\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "preprocessed = image_processor(images=demo_image_one,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"preprocessed:\",preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"preprocessed shape\",preprocessed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接用 OpenFlamingo 抽取特征\n",
    "with torch.no_grad():\n",
    "    vision_x = model.vision_encoder(preprocessed.pixel_values)[1]  # 获取编码器输出，Shape: (1, 256, 1024)\n",
    "\n",
    "# 检查特征维度\n",
    "print(\"Image features shape:\", vision_x.shape)  # 输出应为 (1, 256, 1024) - 一张图片的 (256, 1024) 特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34e59964cf644f0a3d1f1b75db54f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import  AutoProcessor,IdeficsForVisionText2Text\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "checkpoint = \"/data1/pyz/model_weight/idefics-9b\"\n",
    "model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed shape torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "preprocessed = processor.image_processor(demo_image_one,return_tensors=\"pt\")\n",
    "\n",
    "print(\"preprocessed shape\",preprocessed.shape) # preprocessed shape torch.Size([1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IdeficsForVisionText2Text' object has no attribute 'vision_encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     vision_x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_encoder\u001b[49m(preprocessed\u001b[38;5;241m.\u001b[39mpixel_values)[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# 获取编码器输出，Shape: (1, 256, 1024)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 检查特征维度\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage features shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, vision_x\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.conda/envs/openflamingo/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IdeficsForVisionText2Text' object has no attribute 'vision_encoder'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"User:\",\n",
    "          demo_image_one,\n",
    "           \"Classify the following image into a single category.\\nAssistant: vegetables.\\n\",\n",
    "           \"User:\",\n",
    "          demo_image_one,\n",
    "           \"Classify the following image into a single category.\\nAssistant: vegetables.\\n\",\n",
    "           \"User:\",\n",
    "           demo_image_one,\n",
    "           \"Classify the following image into a single category.\\nAssistant:\"\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = [demo_image_one,demo_image_one,demo_image_one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed shape torch.Size([3, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "preprocessed = processor.image_processor(image_list)\n",
    "print(\"preprocessed shape\",preprocessed.shape) # preprocessed shape torch.Size([3, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"pixel_values\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=20,\n",
    "    num_beams=1,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreedySearchDecoderOnlyOutput(sequences=tensor([[    1,  4911, 29901, 32000, 32001, 32000,  4134,  1598,   278,  1494,\n",
      "          1967,   964,   263,  2323,  7663, 29889,    13,  7900, 22137, 29901,\n",
      "         18655,  1849, 29889,    13,  2659, 29901, 32000, 32001, 32000,  4134,\n",
      "          1598,   278,  1494,  1967,   964,   263,  2323,  7663, 29889,    13,\n",
      "          7900, 22137, 29901, 18655,  1849, 29889,    13,  2659, 29901, 32000,\n",
      "         32001, 32000,  4134,  1598,   278,  1494,  1967,   964,   263,  2323,\n",
      "          7663, 29889,    13,  7900, 22137, 29901, 18655,  1849, 29889,    13,\n",
      "          2659, 29901, 32000, 32001, 32000,  4134,  1598,   278,  1494,  1967,\n",
      "           964,   263,  2323,  7663, 29889,    13]], device='cuda:0'), scores=(tensor([[-11.8125, -16.5000,  -3.5625,  ...,  -9.6250,  -1.1016,  -6.0312]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.2500, -13.6250,  -5.4688,  ...,  -8.1875,  -5.1875,  -6.2500]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.8750, -15.1875,  -4.3438,  ...,  -9.7500,  -0.9453,  -6.9688]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.7500, -14.5625,  -0.5586,  ...,  -9.5000,   2.6562,  -6.0000]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -8.3750, -14.7500,  -2.5312,  ...,  -7.4375,  -0.5156,  -7.9375]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.5000, -12.8750,  -4.5312,  ...,  -8.8750,  -2.3125,  -7.3750]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[-10.1250, -14.6875,   0.1113,  ...,  -9.2500,   6.3125,  -5.8750]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.8750, -14.1250,  -6.0000,  ...,  -9.0625,  -5.2188,   6.1875]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.2500, -14.6250,  -4.7812,  ...,  -8.8750,   6.1250,  -7.1250]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[-10.6875, -12.5000,   1.8281,  ...,  -8.3750,  -2.8594,   5.2188]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -8.8750, -14.5000,  -5.6250,  ...,  -9.3125,  -5.4062,  -7.2188]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.5000, -16.7500,  -6.2812,  ...,  -9.0000,  -5.7500,  -7.6562]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[-10.0625, -14.0000,  -5.3750,  ...,  -9.1875,  -4.3125,  -6.1250]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.9375, -15.4375,  -5.6250,  ...,  -9.1875,  -5.0625,  -6.7188]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.5625, -16.3750,  -5.5625,  ..., -10.4375,  -5.4062,  -8.1250]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.5625, -15.9375,  -5.5000,  ...,  -8.8750,  -6.0938,  -6.7188]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.5000, -15.1250,  -5.2188,  ...,  -9.1250,  -5.1875,  -5.1875]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.8750, -14.7500,  -4.5000,  ..., -10.0000,  -4.0625,  -5.7188]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.6250, -15.3750,  -4.3750,  ...,  -9.3750,  -2.9062,  -6.8125]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), tensor([[ -9.6250, -15.1250,  -1.1719,  ...,  -9.1250,   0.8008,  -5.9688]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)), attentions=None, hidden_states=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_auto_class', '_backward_compatibility_gradient_checkpointing', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_convert_head_mask_to_5d', '_create_repo', '_dispatch_accelerate_model', '_expand_inputs_for_generation', '_extract_past_from_model_output', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_from_config', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_decoder_start_token_id', '_get_files_timestamps', '_get_generation_mode', '_get_logits_processor', '_get_logits_warper', '_get_name', '_get_resized_embeddings', '_get_resized_lm_head', '_get_stopping_criteria', '_hf_peft_config_loaded', '_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_init_weights', '_initialize_weights', '_is_full_backward_hook', '_is_hf_initialized', '_keep_in_fp32_modules', '_keys_to_ignore_on_load_missing', '_keys_to_ignore_on_load_unexpected', '_keys_to_ignore_on_save', '_load_from_state_dict', '_load_pretrained_model', '_load_pretrained_model_low_mem', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_initialize_input_ids_for_generation', '_maybe_warn_non_full_backward_hook', '_merge_criteria_processor_list', '_modules', '_named_members', '_no_split_modules', '_non_persistent_buffers_set', '_parameters', '_prepare_attention_mask_for_generation', '_prepare_decoder_input_ids_for_generation', '_prepare_encoder_decoder_kwargs_for_generation', '_prepare_model_inputs', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_reorder_cache', '_replicate_for_data_parallel', '_resize_token_embeddings', '_save_to_state_dict', '_set_default_torch_dtype', '_set_gradient_checkpointing', '_skip_keys_device_placement', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_tie_encoder_decoder_weights', '_tie_or_clone_weights', '_tied_weights_keys', '_update_model_kwargs_for_generation', '_upload_modified_files', '_validate_generated_length', '_validate_model_class', '_validate_model_kwargs', '_version', 'active_adapter', 'add_adapter', 'add_memory_hooks', 'add_module', 'apply', 'assisted_decoding', 'base_model', 'base_model_prefix', 'beam_sample', 'beam_search', 'bfloat16', 'buffers', 'call_super_init', 'can_generate', 'children', 'compute_transition_scores', 'config', 'config_class', 'constrained_beam_search', 'contrastive_search', 'cpu', 'create_extended_attention_mask_for_decoder', 'cuda', 'device', 'disable_adapters', 'disable_input_require_grads', 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'enable_adapters', 'enable_input_require_grads', 'estimate_tokens', 'eval', 'extra_repr', 'float', 'floating_point_ops', 'forward', 'framework', 'from_pretrained', 'generate', 'generation_config', 'get_adapter_state_dict', 'get_buffer', 'get_decoder', 'get_extended_attention_mask', 'get_extra_state', 'get_head_mask', 'get_input_embeddings', 'get_memory_footprint', 'get_output_embeddings', 'get_parameter', 'get_position_embeddings', 'get_submodule', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'greedy_search', 'group_beam_search', 'half', 'init_weights', 'invert_attention_mask', 'ipu', 'is_gradient_checkpointing', 'is_loaded_in_4bit', 'is_loaded_in_8bit', 'is_parallelizable', 'lm_head', 'load_adapter', 'load_state_dict', 'main_input_name', 'model', 'modules', 'name_or_path', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_parameters', 'parameters', 'post_init', 'prepare_inputs_for_generation', 'prune_heads', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_for_auto_class', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings', 'retrieve_modules_from_names', 'reverse_bettertransformer', 'sample', 'save_pretrained', 'set_adapter', 'set_decoder', 'set_extra_state', 'set_input_embeddings', 'set_output_embeddings', 'share_memory', 'state_dict', 'supports_gradient_checkpointing', 'tie_weights', 'to', 'to_bettertransformer', 'to_empty', 'train', 'training', 'type', 'warn_if_padding_and_no_attention_mask', 'warnings_issued', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "# 获取模型的所有方法和属性\n",
    "print(dir(model)) # 'get_input_embeddings', 'get_memory_footprint', 'get_output_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed shape torch.Size([1, 1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "image = [demo_image_one]\n",
    "\n",
    "preprocessed = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "print(\"preprocessed shape\",preprocessed.shape) # preprocessed shape torch.Size([1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed shape torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "preprocessed = preprocessed.squeeze(0)\n",
    "print(\"preprocessed shape\",preprocessed.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IdeficsDecoupledEmbedding(\n",
       "  num_embeddings=32000, num_additional_embeddings=2, embedding_dim=4096, partially_freeze=False\n",
       "  (additional_embedding): Embedding(2, 4096)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.0398,  0.5781,  0.4551,  ...,  0.0835,  0.1226,  0.7891],\n",
       "         [ 0.8828, -0.5586,  0.7578,  ..., -0.1934, -1.0078,  0.1621],\n",
       "         [ 1.5625, -0.2129,  0.6133,  ..., -0.1641, -0.9805,  0.3340],\n",
       "         ...,\n",
       "         [ 0.1660, -0.4004,  0.1973,  ...,  0.2305,  0.3164, -0.1504],\n",
       "         [ 0.1465, -0.3438,  0.1289,  ...,  0.3555,  0.5469, -0.1543],\n",
       "         [-0.0908, -0.1465,  0.2852,  ...,  0.1562,  0.6484, -0.3555]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), pooler_output=tensor([[-0.3574,  0.6016,  0.7070,  ..., -0.0332, -0.0144,  1.4219]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.vision_model(preprocessed.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 257, 1280])\n"
     ]
    }
   ],
   "source": [
    "print(model.model.vision_model(preprocessed.to(device)).last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state = model.model.vision_model(preprocessed.to(device)).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0398,  0.5781,  0.4551,  ...,  0.0835,  0.1226,  0.7891],\n",
      "         [ 0.8828, -0.5586,  0.7578,  ..., -0.1934, -1.0078,  0.1621],\n",
      "         [ 1.5625, -0.2129,  0.6133,  ..., -0.1641, -0.9805,  0.3340],\n",
      "         ...,\n",
      "         [ 0.1660, -0.4004,  0.1973,  ...,  0.2305,  0.3164, -0.1504],\n",
      "         [ 0.1465, -0.3438,  0.1289,  ...,  0.3555,  0.5469, -0.1543],\n",
      "         [-0.0908, -0.1465,  0.2852,  ...,  0.1562,  0.6484, -0.3555]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(last_hidden_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openflamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
